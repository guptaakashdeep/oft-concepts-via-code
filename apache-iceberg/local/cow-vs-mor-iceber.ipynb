{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COW vs MOR in Apache Iceberg Tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark\n",
    "!pip install pyspark==3.5 #change the pyspark version here if you want to run it on anyother version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/venv/lib/python3.8/site-packages/pyspark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/24 16:22:53 WARN Utils: Your hostname, Akashdeeps-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.2 instead (on interface en0)\n",
      "24/01/24 16:22:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/akashdeepgupta/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/akashdeepgupta/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-aa21dd6b-800e-4606-a927-ebad06fab1cc;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/venv/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.2 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.5.0 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      ":: resolution report :: resolve 153ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.2 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.5.0 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-aa21dd6b-800e-4606-a927-ebad06fab1cc\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "24/01/24 16:22:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/24 16:22:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Avro jar to look into Manifest list and manifest file data.\n",
    "# Change the iceberg jar version if pyspark version is other than 3.5 => iceberg-spark-runtime-<pyspark_version>_2.12:1.4.2\n",
    "# update the warehouse path as per your local directory where you want to create Iceberg Tables\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"iceberg-poc\") \\\n",
    "    .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.spark:spark-avro_2.12:3.5.0')\\\n",
    "    .config('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \\\n",
    "    .config('spark.sql.catalog.local','org.apache.iceberg.spark.SparkCatalog') \\\n",
    "    .config('spark.sql.catalog.local.type','hadoop') \\\n",
    "    .config('spark.sql.catalog.local.warehouse','/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading NYC Taxi Trips [data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) for creating Iceberg Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+\n",
      "|VendorID|month|  count|\n",
      "+--------+-----+-------+\n",
      "|       1|    9| 731968|\n",
      "|       2|    9|2113902|\n",
      "|       6|    9|    852|\n",
      "|       2|   10|2617320|\n",
      "|       1|   10| 904463|\n",
      "|       6|   10|    502|\n",
      "+--------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, col\n",
    "# Reading NYC Yellow Taxi Trip Data Sep 2023 data\n",
    "yellow_sep_df = spark.read.parquet(\"../../../nyc-taxi-trips/yellow/sep-2023/\")\n",
    "yellow_oct_df = spark.read.parquet(\"../../../nyc-taxi-trips/yellow/oct-2023/\")\n",
    "# Creating month and year column\n",
    "yellow_sep_df = yellow_sep_df.withColumn(\"month\", lit(9)) \\\n",
    "        .withColumn(\"year\", lit(2023))\n",
    "yellow_oct_df = yellow_oct_df.withColumn(\"month\", lit(10)) \\\n",
    "        .withColumn(\"year\", lit(2023))\n",
    "yellow_df = yellow_sep_df.unionByName(yellow_oct_df)\n",
    "yellow_df.groupBy(\"VendorID\",\"month\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating 2 iceberg tables with same data.\n",
    "- These tables will be have different Table Properties for COW and MOR properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_df.writeTo(\"local.nyc_tlc.yellow_taxi_trips_cow\").partitionedBy(\"year\", \"month\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .tableProperty(\"format-version\", \"2\")\\\n",
    "    .create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "yellow_df.writeTo(\"local.nyc_tlc.yellow_taxi_trips_mor\").partitionedBy(\"year\", \"month\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .tableProperty(\"format-version\", \"2\")\\\n",
    "    .create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cow_table = \"local.nyc_tlc.yellow_taxi_trips_cow\"\n",
    "mor_table = \"local.nyc_tlc.yellow_taxi_trips_mor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up table properties for COW and MOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/24 16:25:56 WARN BaseTransaction: Failed to load metadata for a committed snapshot, skipping clean-up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"ALTER TABLE {cow_table} SET TBLPROPERTIES (\n",
    " 'write.delete.mode'='copy-on-write',\n",
    " 'write.update.mode'='copy-on-write',\n",
    " 'write.merge.mode'='copy-on-write'\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/24 16:25:57 WARN BaseTransaction: Failed to load metadata for a committed snapshot, skipping clean-up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"ALTER TABLE {mor_table} SET TBLPROPERTIES (\n",
    " 'write.delete.mode'='merge-on-read',\n",
    " 'write.update.mode'='merge-on-read',\n",
    " 'write.merge.mode'='merge-on-read'\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Delete Operation\n",
    "\n",
    "Row-level deletes or updates configured as:\n",
    "- `copy-on-write`: rewrites the data files that are impacted on running delete/update operation.\n",
    "- `merge-on-read`: DOESN'T rewrites the data file. Instead,\n",
    "    - In case of DELETE operation, only a Delete File is written that contains the deleted records file_path and the exact postion of deleted record in that file.\n",
    "    - In case of UPDATE operation:\n",
    "        - A Delete File is written that contains the data file path that contains the record is updated along with it's exact position in the file.\n",
    "        - A new Data File that contains only the updated records with updated values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On COW table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from cow table\n",
    "spark.sql(f\"DELETE from {cow_table} where VendorId=6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing COW metadata tables after delete operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id          |operation|manifest_list                                                                                                                                                                         |summary                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+-----------------------+-------------------+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2024-01-24 16:26:48.416|1468630002014670989|8113179530639306271|overwrite|/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_cow/metadata/snap-1468630002014670989-1-b2c17262-0f48-448d-8c11-695c6ea5bdb4.avro|{spark.app.id -> local-1706093575113, added-data-files -> 2, deleted-data-files -> 2, added-records -> 6367653, deleted-records -> 6369007, added-files-size -> 99756800, removed-files-size -> 99780348, changed-partition-count -> 2, total-records -> 6367653, total-files-size -> 99756800, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2024-01-24 16:25:20.528|8113179530639306271|NULL               |append   |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_cow/metadata/snap-8113179530639306271-1-cc747f16-e619-4a54-afd6-4276ed7a5dfd.avro|{spark.app.id -> local-1706093575113, added-data-files -> 2, added-records -> 6369007, added-files-size -> 99780348, changed-partition-count -> 2, total-records -> 6369007, total-files-size -> 99780348, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                                     |\n",
      "+-----------------------+-------------------+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select * from {cow_table}.snapshots\").orderBy(col(\"committed_at\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+\n",
      "|content|path                                                                                                                                                          |length|partition_spec_id|added_snapshot_id  |added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|\n",
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+\n",
      "|0      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_cow/metadata/b2c17262-0f48-448d-8c11-695c6ea5bdb4-m1.avro|9102  |0                |1468630002014670989|2                     |0                        |0                       |0                       |0                          |0                         |\n",
      "|0      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_cow/metadata/b2c17262-0f48-448d-8c11-695c6ea5bdb4-m0.avro|9100  |0                |1468630002014670989|0                     |0                        |2                       |0                       |0                          |0                         |\n",
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the changes in table latest snapshot_id\n",
    "spark.sql(f\"select * from {cow_table}.manifests\").drop(\"partition_summaries\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to look in above output:\n",
    "- `added_data_files_count`, `deleted_data_files_count` and `added_delete_files_count`\n",
    "- This shows that there are data files that are rewritten on performing DELETE operation on table with `COW` table properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+----------+------------+\n",
      "|content|file_path                                                                                                                                                                                   |file_format|partition |record_count|\n",
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+----------+------------+\n",
      "|0      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_cow/data/year=2023/month=9/00000-36-5093cb84-2aaf-4f24-851f-f2a3fd67ffc4-00001.parquet |PARQUET    |{2023, 9} |2845870     |\n",
      "|0      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_cow/data/year=2023/month=10/00001-37-5093cb84-2aaf-4f24-851f-f2a3fd67ffc4-00001.parquet|PARQUET    |{2023, 10}|3521783     |\n",
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the data files present in the table and being currently used by the latest snapshot\n",
    "spark.sql(f\"select content, file_path, file_format, partition,record_count from {cow_table}.files\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On MOR table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from MOR table\n",
    "spark.sql(f\"DELETE from {mor_table} where VendorId=6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing MOR metadata tables after delete operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id          |operation|manifest_list                                                                                                                                                                         |summary                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+-----------------------+-------------------+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2024-01-24 16:39:14.878|1607848476326965355|7732589754235441173|overwrite|/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/metadata/snap-1607848476326965355-1-38dfd8f8-e34e-49fc-8bf2-65b4e4cd7931.avro|{spark.app.id -> local-1706093575113, added-position-delete-files -> 2, added-delete-files -> 2, added-files-size -> 6063, added-position-deletes -> 1354, changed-partition-count -> 2, total-records -> 6369007, total-files-size -> 99786411, total-data-files -> 2, total-delete-files -> 2, total-position-deletes -> 1354, total-equality-deletes -> 0}|\n",
      "|2024-01-24 16:25:29.092|7732589754235441173|NULL               |append   |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/metadata/snap-7732589754235441173-1-ffff5fbc-a61a-48f6-9a79-a47181ac44e8.avro|{spark.app.id -> local-1706093575113, added-data-files -> 2, added-records -> 6369007, added-files-size -> 99780348, changed-partition-count -> 2, total-records -> 6369007, total-files-size -> 99780348, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                         |\n",
      "+-----------------------+-------------------+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the latest_snapshot_id\n",
    "spark.sql(f\"select * from {mor_table}.snapshots\").orderBy(col(\"committed_at\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+\n",
      "|content|path                                                                                                                                                          |length|partition_spec_id|added_snapshot_id  |added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|\n",
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+\n",
      "|1      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/metadata/38dfd8f8-e34e-49fc-8bf2-65b4e4cd7931-m0.avro|8503  |0                |1607848476326965355|0                     |0                        |0                       |2                       |0                          |0                         |\n",
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the latest snapshot_id => there are no data files written and deleted_files are written \n",
    "# i.e. added_data_files_count=0 and added_deleted_files_count=2\n",
    "latest_snapshot_id = 1607848476326965355\n",
    "spark.sql(f\"select * from {mor_table}.manifests where added_snapshot_id={latest_snapshot_id}\").drop(\"partition_summaries\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `manifest` table, `content` column can have 2 possible values:\n",
    "- `0` : represents the manifest file tracking the Data Files.\n",
    "- `1` : represents the manifest file tracking the Delete Files.\n",
    "\n",
    "In the above output, `added_deleted_files_count` shows that 2 delete files are added as a part of DELETE operation and `added_data_files_count`, `deleted_data_files_count` is `0` which means there were no rewriting of data files in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+----------+------------+\n",
      "|content|file_path                                                                                                                                                                                           |file_format|partition |record_count|\n",
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+----------+------------+\n",
      "|0      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-29-2d94390c-70a5-481f-9b11-203c6b6026a4-00001.parquet         |PARQUET    |{2023, 9} |2846722     |\n",
      "|0      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=10/00001-30-2d94390c-70a5-481f-9b11-203c6b6026a4-00001.parquet        |PARQUET    |{2023, 10}|3522285     |\n",
      "|1      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=10/00000-44-e4cc2c1d-c517-4b89-95f2-1597396e193d-00002-deletes.parquet|PARQUET    |{2023, 10}|502         |\n",
      "|1      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-44-e4cc2c1d-c517-4b89-95f2-1597396e193d-00001-deletes.parquet |PARQUET    |{2023, 9} |852         |\n",
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# files table show only the data files that current manifest file is pointing towards\n",
    "spark.sql(f\"select content, file_path, file_format, partition,record_count from {mor_table}.files\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `files` metadata table, `content` column can have 3 possible values: \n",
    "- `0`: represents a Data File.\n",
    "- `1`: represents a Positional Delete File.\n",
    "- `2`: represents a Equality Delete File.\n",
    "\n",
    "In the output above the rows with `1` shows that 2 Positional Delete Files are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|file_path                                                                                                                                                                                  |pos    |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-29-2d94390c-70a5-481f-9b11-203c6b6026a4-00001.parquet|2706579|\n",
      "|/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-29-2d94390c-70a5-481f-9b11-203c6b6026a4-00001.parquet|2707160|\n",
      "|/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-29-2d94390c-70a5-481f-9b11-203c6b6026a4-00001.parquet|2707197|\n",
      "|/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-29-2d94390c-70a5-481f-9b11-203c6b6026a4-00001.parquet|2707198|\n",
      "|/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-29-2d94390c-70a5-481f-9b11-203c6b6026a4-00001.parquet|2707243|\n",
      "|/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-29-2d94390c-70a5-481f-9b11-203c6b6026a4-00001.parquet|2707293|\n",
      "|/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-29-2d94390c-70a5-481f-9b11-203c6b6026a4-00001.parquet|2707294|\n",
      "|/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-29-2d94390c-70a5-481f-9b11-203c6b6026a4-00001.parquet|2707295|\n",
      "|/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-29-2d94390c-70a5-481f-9b11-203c6b6026a4-00001.parquet|2707370|\n",
      "|/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-29-2d94390c-70a5-481f-9b11-203c6b6026a4-00001.parquet|2707423|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the content for one of the deleted_files\n",
    "delete_file_path = \"/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-44-e4cc2c1d-c517-4b89-95f2-1597396e193d-00001-deletes.parquet\"\n",
    "spark.read.parquet(delete_file_path).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Delete Files store the `file_path` of the Data File from which a record is deleted along with the exact position `pos` of this deleted record from that data file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing update operation on MOR table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/24 17:10:09 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing an update operation\n",
    "spark.sql(f\"update {mor_table} set fare_amount = 0 where VendorID=2 and fare_amount < 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2024-01-24 17:10:14.418|3121591312207175607|1607848476326965355|true               |\n",
      "|2024-01-24 16:39:14.878|1607848476326965355|7732589754235441173|true               |\n",
      "|2024-01-24 16:25:29.092|7732589754235441173|NULL               |true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select * from {mor_table}.history\").orderBy(col(\"made_current_at\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id          |operation|manifest_list                                                                                                                                                                         |summary                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+-----------------------+-------------------+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2024-01-24 17:10:14.418|3121591312207175607|1607848476326965355|overwrite|/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/metadata/snap-3121591312207175607-1-61b273ac-b6ba-4985-a5cc-b1b2722533d3.avro|{spark.app.id -> local-1706093575113, added-data-files -> 2, added-position-delete-files -> 2, added-delete-files -> 2, added-records -> 66661, added-files-size -> 1087105, added-position-deletes -> 66661, changed-partition-count -> 2, total-records -> 6435668, total-files-size -> 100873516, total-data-files -> 4, total-delete-files -> 4, total-position-deletes -> 68015, total-equality-deletes -> 0}|\n",
      "|2024-01-24 16:39:14.878|1607848476326965355|7732589754235441173|overwrite|/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/metadata/snap-1607848476326965355-1-38dfd8f8-e34e-49fc-8bf2-65b4e4cd7931.avro|{spark.app.id -> local-1706093575113, added-position-delete-files -> 2, added-delete-files -> 2, added-files-size -> 6063, added-position-deletes -> 1354, changed-partition-count -> 2, total-records -> 6369007, total-files-size -> 99786411, total-data-files -> 2, total-delete-files -> 2, total-position-deletes -> 1354, total-equality-deletes -> 0}                                                     |\n",
      "|2024-01-24 16:25:29.092|7732589754235441173|NULL               |append   |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/metadata/snap-7732589754235441173-1-ffff5fbc-a61a-48f6-9a79-a47181ac44e8.avro|{spark.app.id -> local-1706093575113, added-data-files -> 2, added-records -> 6369007, added-files-size -> 99780348, changed-partition-count -> 2, total-records -> 6369007, total-files-size -> 99780348, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                                              |\n",
      "+-----------------------+-------------------+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select * from {mor_table}.snapshots\").orderBy(col(\"committed_at\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+\n",
      "|content|path                                                                                                                                                          |length|partition_spec_id|added_snapshot_id  |added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|\n",
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+\n",
      "|0      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/metadata/61b273ac-b6ba-4985-a5cc-b1b2722533d3-m0.avro|8978  |0                |3121591312207175607|2                     |0                        |0                       |0                       |0                          |0                         |\n",
      "|1      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/metadata/61b273ac-b6ba-4985-a5cc-b1b2722533d3-m1.avro|8509  |0                |3121591312207175607|0                     |0                        |0                       |2                       |0                          |0                         |\n",
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latest_snapshot_id = 3121591312207175607\n",
    "spark.sql(f\"select * from {mor_table}.manifests\").drop(\"partition_summaries\").filter(col(\"added_snapshot_id\") == latest_snapshot_id).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the `manifest` metadata table output above for the current snapshot after UPDATE operation:\n",
    "- In `content` = `0` row, it can be seen that there are 2 data files that has been added, `added_data_files_count` = `2`. These are the Data files that contains the records with updated values\n",
    "- In `content` = `1` row, it can be seen that there are 2 Delete Files that has been added, `added_delete_files_count` = `2`. These are the Delete files that contains the file_path of data files and position of the deleted records within this data file.\n",
    "\n",
    "This shows that there are no data files rewritten as there are 0 `delete_data_files_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|VendorID|month|count|\n",
      "+--------+-----+-----+\n",
      "|       2|    9|29562|\n",
      "|       2|   10|37099|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# impacted records from update operation\n",
    "yellow_df.filter((col(\"fare_amount\") < 0) & (col(\"VendorID\") == 2)).groupBy(\"VendorID\", \"month\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+----------+------------+\n",
      "|content|file_path                                                                                                                                                                                           |file_format|partition |record_count|\n",
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+----------+------------+\n",
      "|0      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-86-ec64a270-e3b3-47a9-887f-f848abe94fd9-00001.parquet         |PARQUET    |{2023, 9} |29562       |\n",
      "|0      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=10/00001-87-ec64a270-e3b3-47a9-887f-f848abe94fd9-00001.parquet        |PARQUET    |{2023, 10}|37099       |\n",
      "|0      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-35-9daf8219-ec60-49b6-bfb5-89545676c926-00001.parquet         |PARQUET    |{2023, 9} |2846722     |\n",
      "|0      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=10/00001-36-9daf8219-ec60-49b6-bfb5-89545676c926-00001.parquet        |PARQUET    |{2023, 10}|3522285     |\n",
      "|1      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=10/00000-86-ec64a270-e3b3-47a9-887f-f848abe94fd9-00002-deletes.parquet|PARQUET    |{2023, 10}|37099       |\n",
      "|1      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-86-ec64a270-e3b3-47a9-887f-f848abe94fd9-00001-deletes.parquet |PARQUET    |{2023, 9} |29562       |\n",
      "|1      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=10/00000-55-7aaec06c-15ca-4ea4-a8cd-52a5eb0903c4-00002-deletes.parquet|PARQUET    |{2023, 10}|502         |\n",
      "|1      |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips_mor/data/year=2023/month=9/00000-55-7aaec06c-15ca-4ea4-a8cd-52a5eb0903c4-00001-deletes.parquet |PARQUET    |{2023, 9} |852         |\n",
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select content, file_path, file_format, partition,record_count from {mor_table}.files\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Row#1 and Row#2 with `content` = `0`, shows that the same number of records are present in the data files as per the previous data file output.\n",
    "- Row#5 and Row#6 with `content` = `1`, shows that the same number of records present in the Delete Files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
